{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This homework has 4 questions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import (\n",
    "    Lasso,\n",
    "    LinearRegression\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate some data for a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000  # number of observations in the data\n",
    "p = 100   # number of predictors in the data\n",
    "k = 10    # number of relevant predictors in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, coef = make_regression(n_samples=n,\n",
    "                             n_features=p,\n",
    "                             n_informative=k,\n",
    "                             noise=0.005,  # add a little gaussian noise to the data\n",
    "                             coef=True,\n",
    "                             random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that *we* generate the data here, therefore we know the \"truth\" about the data generating process: only 10 predictors are relevant out of the $p = 100$ predictors available in the data.\n",
    "\n",
    "In fact, here are the true coefficients of the data generating process (only the 10 non-zero coefficients are printed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_6: 8.88918861576965\n",
      "beta_15: 19.365117777553486\n",
      "beta_47: 48.24477094388765\n",
      "beta_56: 82.84660305904961\n",
      "beta_66: 1.8559304095025264\n",
      "beta_73: 29.506960083972224\n",
      "beta_80: 68.28011328758366\n",
      "beta_85: 78.46795175040386\n",
      "beta_87: 25.37933359399561\n",
      "beta_90: 57.00013284018336\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join([f'beta_{i}: {coef[i]}' for i in range(p) if coef[i] != 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would expect that given a sample of $n = 1000$ data points from this data generating process, a LASSO model will be able to select exactly 10 predictors and discard the remaining ones (i.e. set their $\\widehat{\\beta}$ estimates to 0).\n",
    "\n",
    "Let's verify empirically whether this is in fact the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (3 points)\n",
    "\n",
    "Set up a model pipeline that uses the `StandardScaler` to normalize the data and feed them to a `Lasso` model.\n",
    "\n",
    "- For this, you can use the `make_pipeline` utility in the `sklearn` library.\n",
    "\n",
    "- Assign the pipeline object to a variable named `lasso_pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_pipeline = sklearn.pipeline.make_pipeline(sklearn.preprocessing.StandardScaler(),\n",
    "                                                sklearn.linear_model.Lasso())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a grid of 50 (by default) equally spaced (in the log scale) candidate values for the regularization parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_candidates = np.logspace(-4.0, -2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (3 points)\n",
    "\n",
    "- Use `GridSearchCV` with 2-fold cross-validation to find the best value of $\\alpha$ for these data.\n",
    "\n",
    "- Assign the fitted `GridSearchCV` object to a variable named `alpha_search`.\n",
    "\n",
    "This will look something like\n",
    "\n",
    "```\n",
    "GridSearchCV(lasso_pipeline,\n",
    "             {'lasso__alpha': alpha_candidates},\n",
    "             cv=<number of folds>).fit(<data>)\n",
    "```\n",
    "\n",
    "where `<number of folds>` and `<data>` are placeholders for code that you need to write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_search = sklearn.model_selection.GridSearchCV(lasso_pipeline,\n",
    "                                                    {'lasso__alpha': alpha_candidates},\n",
    "                                                    cv=2).fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we count how many of the model coefficients for the best LASSO model (i.e. the one\n",
    "corresponding to the optimal value of $\\alpha$ selected via cross-validation) are non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(np.sum(alpha_search.best_estimator_[1].coef_ != 0))\n",
    "except NameError:\n",
    "    print('The object `alpha_search` does not exist! Did you forget to create it in Question 2?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, *which* coefficients are non-zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_hat_6: 8.687958221359063\n",
      "beta_hat_15: 19.726802408850183\n",
      "beta_hat_47: 48.347385382805285\n",
      "beta_hat_56: 84.70297559430583\n",
      "beta_hat_66: 1.905424924453863\n",
      "beta_hat_73: 28.963729026956308\n",
      "beta_hat_80: 62.93734556951447\n",
      "beta_hat_85: 76.56912272269763\n",
      "beta_hat_87: 25.280346286439105\n",
      "beta_hat_90: 58.505816871254616\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('\\n'.join([f'beta_hat_{i}: {alpha_search.best_estimator_[1].coef_[i]}'\n",
    "           for i in range(p)\n",
    "           if alpha_search.best_estimator_[1].coef_[i] != 0.0]))\n",
    "except NameError:\n",
    "    print('The object `alpha_search` does not exist! Did you forget to create it in Question 2?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, here are the true $\\beta$ values that these coefficients are trying to estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_6: 8.88918861576965\n",
      "beta_15: 19.365117777553486\n",
      "beta_47: 48.24477094388765\n",
      "beta_56: 82.84660305904961\n",
      "beta_66: 1.8559304095025264\n",
      "beta_73: 29.506960083972224\n",
      "beta_80: 68.28011328758366\n",
      "beta_85: 78.46795175040386\n",
      "beta_87: 25.37933359399561\n",
      "beta_90: 57.00013284018336\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join([f'beta_{i}: {coef[i]}' for i in range(p) if coef[i] != 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what would happen if we fitted a conventional multiple regression model on these data instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (2 points)\n",
    "\n",
    "Count how many coefficients are non-zero in the multiple linear regression model.\n",
    "\n",
    "You can use the same code as above with minor modifications:\n",
    "\n",
    "```\n",
    "np.sum(<model>.coef_ != 0)\n",
    "\n",
    "```\n",
    "\n",
    "`<model>` is a placeholder for code that you need to write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    non_zero_coefficients = np.sum(linear_regression.coef_ != 0)\n",
    "    print(non_zero_coefficients)\n",
    "except NameError:\n",
    "    print('The object `linear_regression` does not exist! Did you forget to create it in Question 3?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (2 point)\n",
    "\n",
    "Which of the two models learns best the data-generating process that produced the sample data? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10 relevant predictors out of p=100 of total predictors. \n",
    "This means 90 of the features are not informative to the target `y`.\n",
    "\n",
    "- The Lasso model is better in learning the data-generating process because \n",
    "it ignores the non-informative 90 features.\n",
    "Lasso will set non-informative coefficients to zero, as seen above.\n",
    "When we focus on the 10 relevant predictors, it is better for learning.\n",
    "as defined for lasso models: \n",
    "    \"The `alpha` parameter controls the strength of regularization. A grid search over `alpha` values was \n",
    "    conducted to find the best setting that minimizes cross-validated loss. This process helps in identifying \n",
    "    a model that captures the underlying data-generating process by keeping the model complexity \n",
    "    (number of non-zero coefficients) in check.\"\n",
    "- The Linear Regression model is worse because\n",
    "it uses all 100 predictors, including the 90 that are not informative, as seen above. \n",
    "We have the possibility of overfitting when we have 100 predictors, not 10. which leads to poor prediction for new data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
